# Running Kubernetes on Multiple Clouds and Cluster Federation

In this chapter we'll take it to the next level, with running Kubernetes on multiple clouds, multiple clusters  and cluster federation. A Kubernetes cluster is a closely-knit unit where all the components run in relative proximity and are connected by a fast network (typically a physical data center or cloud provider availability zone). This is great for many use cases, but there are several important use cases where systems need to scale beyond a single cluster. One approach to address this use case is Kubernetes federation, which  is a methodical way to combine multiple Kubernetes clusters and interact with them as a single entity. Another approach taken by the [Gardener](https://gardener.cloud/) project is to provide and abstraction around managing multiple separate Kubernetes clusters. 

The topics we will cover include the following:

- The history of cluster federation on Kubernetes
- Understanding cluster federation
- A deep dive into what cluster federation is all about
- How to prepare, configure, and manage a cluster federation
- How to run a federated workload across multiple clusters
- Introduction to the Gardener project


# The history of cluster federation on Kubernetes

Before jumping into the details let's get some historical context. It's funny to talk about the history of a project like Kubernetes that didn't even exist five years ago, but the pace of development and the large number of contributors took Kubernetes through accelerated evolution. This is especially relevant for Kubernetes federation. 

In March 2015 the first revision of the [Kubernetes Cluster Federation](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multicluster/federation.md) proposal was published. It was fondly nicknamed "Ubernetes" back then. The basic idea was to reuse the existing Kubernetes APIs to manage multiple clusters. This proposal now called Federation V1 went through several rounds of revision and implementation, but never reached general availability and it is considered deprecated at this point. The SIG cluster workgroup realized that the multi-cluster problem is more complicated than initially perceived. There are many ways to skin this particular cat and there is no one size fits all solution.

The new direction for cluster federation is dedicated APIs for federation. In the rest of the chapter we will consider the Federation V2 design. Note that the current status is considered Alpha, so I don't recommend putting it to use in production without significant consideration.

# Understanding cluster federation

Cluster federation is conceptually simple. You aggregate multiple Kubernetes clusters and treat them as a single logical cluster. There is a federation control plane that presents to clients a single unified view of the system.

The following diagram demonstrates the big picture of Kubernetes cluster federation. The pink box is a host cluster that runs the federation  APIs and the green boxes are member clusters:

![](images/chapter 11 - big picture.png)
**images/chapter 11 - big picture.png**

The federation control plane consists of a federation API server and a federation controller manager that collaborate. The federated API server forwards requests to all the clusters in the federation. In addition, the federated controller manager performs the duties of the controller manager across all clusters by routing requests to the individual federation cluster members' changes. In practice, cluster federation is not trivial and can't be totally abstracted away. Cross-pod communication and data transfer may suddenly incur a massive latency and cost overhead. Let's look at the use cases for cluster federation first, understand how the federated components and resources work, and then examine the hard parts: location affinity, cross-cluster scheduling, and federated data access.

## Important use cases for cluster federation

There are four categories of use cases that benefit from cluster federation.

### Capacity overflow

The public cloud platforms such as AWS, GCE, and Azure are great and provide many benefits, but they are not cheap. Many large organizations have invested a lot in their own data centers. Other organizations work with private service providers such as OVS, Rackspace, or Digital Ocean. If you have the operational capacity to manage and operate infrastructure on your own it makes a lot of economic sense to run your Kubernetes cluster on your infrastructure rather than in the cloud. But what if some of your workloads fluctuate and for a relatively short amount of time require a lot more capacity?

For example, your system may be hit especially hard on the weekends or maybe during holidays. The traditional approach is to just provision extra capacity. But in many dynamic situations, it is not easy. With capacity overflow, you can run the bulk of your work in a Kubernetes cluster running on an on-premise data center or with a private service provider and have a secondary cloud-based Kubernetes cluster running on one of the big platform providers. Most of the time, the cloud-based cluster will be shut down (stopped instances), but when the need arises you can elastically add capacity to your system by starting some stopped instances. Kubernetes cluster federation can make this configuration relatively straightforward. It eliminates a lot of headaches about capacity planning and paying for hardware that's not used most of the time.

This approach is sometimes called "cloud bursting".

### Sensitive workloads

This is almost the opposite of capacity overflow. Maybe you've embraced the cloud native lifestyle and your entire system runs on the cloud, but some data or workloads deal with sensitive information. Regulatory compliance or your organization's security policies may dictate that those data and workloads must run in an environment that's fully controlled by you. Your sensitive data and workloads may be subject to external auditing. It may be critical to ensure no information ever leaks from the private Kubernetes cluster to the cloud-based Kubernetes cluster. But it may be desirable to have visibility into the public cluster and the ability to launch non-sensitive workloads from the private cluster to the cloud-based cluster. If the nature of a workload can change dynamically from non-sensitive to sensitive then it needs to be addressed by coming up with a proper policy and implementation. For example, you may prevent workloads from changing their nature. Alternatively, you may migrate a workload that suddenly became sensitive and ensure that it doesn't run on the cloud-based cluster anymore. Another important instance is national compliance, where certain data is required by law to remain and be accessed only from a designated geographical region (typically a country). In this case, a cluster must be created in that geographical region.

### Avoiding vendor lock-in

Large organizations often prefer to have options and not be tied to a single provider. The risk is often too great, because the provider may shut down or be unable to provide the same level of service. Having multiple providers is often good for negotiating prices, too. Kubernetes is designed to be vendor-agnostic. You can run it on different cloud platforms, private service providers, and on-premise data centers.

However, this is not trivial. If you want to be sure that you are able to switch providers quickly or shift some workloads from one provider to the next, you should already be running your system on multiple providers. You can do it yourself or there are some companies that provide the service of running Kubernetes transparently on multiple providers. Since different providers run different data centers, you automatically get some redundancy and protection from vendor-wide outages.

### Geo-distributing high availability

High availability means that a service will remain available to users even when some parts of the system fail. In the context of a federated Kubernetes cluster, the scope of failure is an entire cluster, which is typically due to problems with the physical data center hosting the cluster, or perhaps a wider issue with the platform provider. The key to high-availability is redundancy. Geo-distributed redundancy means having multiple clusters running in different locations. It may be different availability zones of the same cloud provider, different regions of the same cloud provider, or even different cloud providers altogether (see the Avoiding vendor lock-in section). There are many issues to address when it comes to running a cluster federation with redundancy. We'll discuss some of these issues later. Assuming that the technical and organizational issues have been resolved, high availability will allow the switching of traffic from a failed cluster to another cluster. This should be transparent to the users up to a point (delay during switchover and some in-flight requests or tasks may disappear or fail). The system administrators may need to take extra steps to support the switchover and to deal with the original cluster failure.


Now, that we understand why multi-cluster federation is such an important aspect of Kubernetes it's time to dive in.

## Learning the basics of Kubernetes Federation

Kubernetes federation is a complex topic and we will ease our way into it. In this section, we will first look at some basic concepts, then at the building blocks of the API and finally at the supported features and capabilities. 

### Defining basic concepts

Let's start our journey into the Kubernetes federation with some basic concepts and terminology. The following table describes the most important concepts and terms 

|----------------------|--------------------------------------------------------------------------------|
| Concept              | Description                                                                    |
|----------------------|--------------------------------------------------------------------------------|
| Federate             | Create a common interface to a pool of clusters in order                       |
|                      |   to deploy Kubernetes applications across those clusters.                     |
|----------------------|--------------------------------------------------------------------------------|
| KubeFed              | The API and control plane of Kubernetes Cluster Federation.                    |
|----------------------|--------------------------------------------------------------------------------|
| Host Cluster         | A cluster that exposes the KubeFed API and runs the KubeFed control plane.     |
|----------------------|--------------------------------------------------------------------------------|
| Cluster Registration | The process of adding a new cluster to the federation.                         |
|----------------------|--------------------------------------------------------------------------------|
| Member Cluster       | A cluster which is registered with the KubeFed API and that KubeFed            |      
|                      | controllers have authentication credentials for. The Host Cluster can          |
|                      | be a Member Cluster too.                                                       |
|----------------------|--------------------------------------------------------------------------------|
| ServiceDNSRecord     | A resource that associates one or more Kubernetes Service resources            |
|                      | with a scheme for constructing DNS resource records  for the Service.          |
|----------------------|--------------------------------------------------------------------------------|
| IngressDNSRecord     | A resource that associates one or more Kubernetes Ingress resources            |
|                      | with a scheme for constructing DNS resource records for the Ingress.           |
|----------------------|--------------------------------------------------------------------------------|
| Endpoint             | A resource that represents a Domain Name System (DNS) resource record.         |
|----------------------|--------------------------------------------------------------------------------|
| DNSEndpoint          | A Custom Resource wrapper for the Endpoint resource.                           |
|----------------------|--------------------------------------------------------------------------------|


In addition to these concepts the architecture is based on three building blocks.

### Federation Building Blocks

A federation is responsible for a given set of API types (Kubernetes resources) it manages and distributes into a set of member clusters. For each API type there are common dedicated resources that the federation control plane uses to keep their state:

- Federated<Type>Template: stores the basic specification of the managed resource 
- Federated<Type>Placement:  type holds the specification of the clusters the resource should be distributed to
- Federated<Type>Overrides: This optional resource can specify how the Template resource should behave on specific clusters.

These types are all associated by name. For example, for the ReplicaSet resource there are FederatedReplicaSetTemplate, FederatedReplicaSetPlacement and FederatedReplicaSetOverrids.


In addition the following elements can be used by higher-level APIs to customize and control the behavior of the federation:

- Status: collect the status of resources distributed by KubeFed across all federated clusters
- Policy: determine which subset of clusters a resource is allowed to be distributed to
- Scheduling: decide how workloads should be distributed across different clusters

The following diagram illustrates the full lifecycle and interaction of all the elements of  Kubernetes federation:

![](images/chapter 11 - federation concepts.png)
**images/chapter 11 - federation concepts.png**

### Federation Features

These concepts and the foundational building blocks are used to implement the following features:
 
- Push propagation of arbitrary types to remote clusters
- A command-line interface program called `kubefedctl` to interact with the KubeFed API
- Generating KubeFed APIs without writing code
- Multicluster Service DNS via external-dns
- Multicluster Ingress DNS via external-dns
- Replica Scheduling Preferences 


## The KubeFed control plane

The KubeFed control plane requires Kubernetes 1.13 or later. It consists of two components that together enable a federation of Kubernetes clusters to appear and function as a single unified Kubernetes cluster.


### Federation API server

The federation API server is managing the Kubernetes clusters that together comprise the federation. It manages the federation state (which clusters are part of the federation) in an etcd database the same as a regular Kubernetes cluster, but the state it keeps is just which clusters are members of the federation. The state of each cluster is stored in the etcd database of that cluster. The main purpose of the federation API server is to interact with the federation controller manager and route requests to the federation member clusters. The federation members don't need to know they are part of a federation: they just work the same.


### Federation controller manager

The federation controller manager makes sure the federation's desired state matches the actual state. It forwards any necessary changes to the relevant cluster or clusters. The federated controller manager binary contains multiple controllers for all the different federated resources we'll cover later in the chapter. The control logic is similar, though: observes changes and brings cluster state to the desired state when they deviate. This is done for each member in the cluster federation.

The following diagram demonstrates this perpetual control loop:

![](images/chapter 11 - perpetual control loop.png)
**images/chapter 11 - perpetual control loop.png**

## The hard parts

So far, federation seems almost straightforward. KubeFed does the heavy lifting for you. You just group a bunch of clusters together, access them through the control plane, and everything just gets replicated to all the clusters. But there are hard and difficult factors and basic concepts that complicate this simplified view. Much of the power of Kubernetes is derived from its ability to do a lot of work behind the scenes. Within a single cluster deployed fully in a single physical data center or availability zone where all the components are connected with a fast network, Kubernetes is very effective on its own. In a Kubernetes cluster federation, the situation is different. Latency, data transfer costs, and moving pods between clusters all have different trade-offs. Depending on the use case, making federation work may require extra attention, planning, and maintenance on the part of the system designers and operators. In addition, some of the federated resources are not as mature as their local counterparts, and that adds more uncertainty.

### Federated unit of work

The unit of work in a Kubernetes cluster is the pod. You can't break a pod in Kubernetes. The entire pod will always be deployed together and be subject to the same lifecycle treatment. Should the pod remain the unit of work for a cluster federation? Maybe it makes more sense to be able to associate a bigger unit, such as a whole ReplicaSet, deployment, or service with a specific cluster. If the cluster fails, the entire ReplicaSet, deployment, or service is scheduled to a different cluster. How about a collection of tightly coupled ReplicaSets? The answers to these questions are not always easy and may even change dynamically as the system evolves.

### Location affinity

Location affinity is a major concern. When can pods be distributed across clusters? What are the relationships between those pods? Are there any requirements for affinity between pods or pods and other resources, such as storage? There are
several major categories:

- Strictly-coupled
- Loosely-coupled
- Preferentially-coupled
- Strictly-decoupled
- Uniformly-spread

When designing the system and how to allocate and schedule services and pods across the federation it's important to make sure the location affinity requirements are always respected.

#### Strictly-coupled

The strictly-coupled requirement applies to applications where the pods must be in the same cluster. If you partition the pods, the application will fail (perhaps due to real-time requirements that can't be met when networking across clusters) or the cost may be too high (pods accessing a lot of local data). The only way to move such tightly coupled applications to another cluster is to start a complete copy (including data) on another cluster and then shut down the application on the current cluster. If the data is too large, the application may practically be immovable and sensitive to catastrophic failure. This is the most difficult situation to deal with, and if possible you should architect your system to avoid the strictly-coupled requirement.

#### Loosely-coupled

Loosely-coupled applications are best when the workload is embarrassingly parallel and each pod doesn't need to know about the other pods or access a lot of data. In these situations, pods can be scheduled to clusters just based on capacity and resource utilization across the federation. If necessary, pods can be moved from one cluster to another without problems. For example, a stateless validation service that performs some calculation and gets all its input in the request itself and doesn't query or write any federation-wide data. It just validates its input and returns a valid/invalid verdict to the caller.

#### Preferentially-coupled

Preferentially-coupled applications perform better when all the pods are in the same cluster or the pods and the data are co-located, but it is not a hard requirement. For example, it could work with applications that require only eventual consistency, where some federation-wide application periodically synchronizes the application state across all clusters. In these cases, allocation is done explicitly to one cluster, but leaves a safety hatch for running or migrating to other clusters under stress.

#### Strictly-decoupled

Some services have fault isolation or high availability requirements that force partitioning across clusters. There is no point running three replicas of a critical service if all replicas might end up scheduled to the same cluster, because that
cluster just becomes an ad hoc single point of failure (SPOF).

#### Uniformly-spread

Uniformly-spread is when an instance of a service, ReplicaSet, or pod must run on each cluster. It is similar to DaemonSet, but instead of ensuring there is one instance on each node, it's one per cluster. A good example is a Redis cache backed up by some external persistent storage. The pods in each cluster should have their own cluster-local Redis cache to avoid accessing the central storage that may be slower or become a bottleneck. On the other hand, there is no need for more than one Redis service per cluster (it could be distributed across several pods in the same cluster).

### Cross-cluster scheduling

Cross-cluster scheduling goes hand-in-hand with location affinity. When a new pod is created or an existing pod fails and a replacement needs to be scheduled, where should it go? The current cluster federation doesn't handle all the scenarios and options for location affinity we mentioned earlier. At this point, cluster federation handles the loosely-coupled (including weighted distribution) and strictly-coupled (by making sure the number of replicas matches the number of clusters) categories well. Anything else will require that you don't use cluster federation. You'll have to add your own custom federation layer that takes more specialized concerns into account and can accommodate more intricate scheduling use cases.

### Federated data access

This is a tough problem. If you have a lot of data and pods running in multiple clusters (possibly on different continents) and need to access it quickly, then you have several unpleasant options:

- Replicate your data to each cluster (slow to replicate, expensive to transfer, expensive to store, and complicated to sync and deal with errors)
- Access the data remotely (slow to access, expensive on each access, can be a SPOF)
- Sophisticated hybrid solution with per-cluster caching of some of the hottest data (complicated, stale data, and you still need to transfer a lot of data)

### Federated auto-scaling

There is currently no support for federated auto-calling. There are two dimensions of scaling that can be utilized, as well as a combination:

- Per cluster scaling
- Adding/removing clusters from the federation
- Hybrid approach

Consider the relatively simple scenario of a loosely coupled application running on three clusters with five pods in each cluster. At some point, 15 pods can't handle the load anymore. We need to add more capacity. We can increase the number of pods per cluster, but if we do it at the federation level than we will have six pods running in each cluster. We've increased the federation capacity by three pods, when only one pod is needed. Of course, if you have more clusters the problem gets worse. Another option is to pick a cluster and just change its capacity. This is possible with annotations, but now we're explicitly managing capacity across the federation. It can get complicated very quickly if we have lots of clusters running hundreds of services with dynamically changing requirements.

Adding a whole new cluster is even more complicated. Where should we add the new cluster? There is no requirement for extra availability that can guide the decision. It is just about extra capacity. Creating a new cluster also often requires complicated first time setup that may take days to approve various quotas on public cloud platforms. The hybrid approach increases the capacity of existing clusters in the federation until reaching some threshold and then starts adding new clusters. The benefit of this approach is that when you're getting closer to capacity limit per cluster you start preparing new clusters that will be ready to go when necessary. Other than that, it requires a lot of effort and you pay in increased complexity for the flexibility and scalability.

# Managing a Kubernetes cluster federation

Managing a Kubernetes cluster federation involves many activities above and beyond managing a single cluster. You need to consider cascading resource deletion, load balancing across clusters, failover across clusters, federated service discovery, and federated discovery. Let's go over the various activities in detail. Note that due to the Alpha status of KubeFed this should not be considered a step by step guide to follow. The goal here is to get a sense of what's involved in the management of multiple Kubernetes clusters as a federation. 

## Installing kubefedctl

The best way to interact with KubeFed is through the kubefedctl CLI. Here are the instructions to install the latest release of kubefedctl for MacOS:

```
VERSION=0.1.0-rc6
OS=darwin
ARCH=amd64
curl -LO https://github.com/kubernetes-sigs/kubefed/releases/download/v${VERSION}/kubefedctl-${VERSION}-${OS}-${ARCH}.tgz
tar -zxvf kubefedctl-*.tgz
chmod u+x kubefedctl
sudo mv kubefedctl /usr/local/bin/ 
```

If typing kubefedctl is too much of a burden you can alias it like I did:

```
alias kf='kubefedctl`
```

To verify it's installed correctly just run it and you see the following:

```
$ kf
kubefedctl controls a Kubernetes Cluster Federation. Find more information at https://sigs.k8s.io/kubefed.

Usage:
  kubefedctl [flags]
  kubefedctl [command]

Available Commands:
  disable            Disables propagation of a Kubernetes API type
  enable             Enables propagation of a Kubernetes API type
  federate           Federate creates a federated resource from a kubernetes resource
  help               Help about any command
  join               Register a cluster with a KubeFed control plane
  orphaning-deletion Manage orphaning delete policy
  unjoin             Remove the registration of a cluster from a KubeFed control plane
  version            Print the version info

Flags:
      --alsologtostderr                  log to standard error as well as files
  -h, --help                             help for kubefedctl
      --log-backtrace-at traceLocation   when logging hits line file:N, emit a stack trace (default :0)
      --log-dir string                   If non-empty, write log files in this directory
      --log-file string                  If non-empty, use this log file
      --log-flush-frequency duration     Maximum number of seconds between log flushes (default 5s)
      --logtostderr                      log to standard error instead of files (default true)
      --skip-headers                     If true, avoid header prefixes in the log messages
      --stderrthreshold severity         logs at or above this threshold go to stderr
  -v, --v Level                          number for the log level verbosity
      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging

Use "kubefedctl [command] --help" for more information about a command.
```

The next step is to create some clusters that will form our federation.

## Creating clusters

KubeFed supports officially four Kubernetes environments:
- KinD (Kubernetes in Docker)
- Minikube
- GKE
- IBM Cloud

The KinD environment is used by KubeFed end to end tests. But, minikube is the easiest to set up for playing around. Here are the instructions for creating two minikube clusters:

```
minikube start -p cluster-1
minikube start -p cluster-2
```  

Then in each cluster verify all the pods are running before moving forward

## Configuring the Host Cluster

OK. It's time to install the KubeFed control plane in your host cluster. The KubeFed project provides a convenient Helm chart for the task. Unfortunately, KubeFed doesn't support Helm 3 yet, because they use an outdated annotation (crd-instal hook). You probably have Helm 3 installed, but you should install Helm 2 as well if you want to try using KubeFed. Since Helm 2 uses Tiller in the cluster you need to create a service account for tiller and give it admin permissions so it can install the KubeFed control plane securely:

```
$ cat << EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF

$ helm init --service-account tiller
```   

Next, we need to add the KubeFed chart repository

```
$ helm repo add kubefed-charts https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts
"kubefed-charts" has been added to your repositories

$ helm repo list
NAME          	URL
stable        	https://kubernetes-charts.storage.googleapis.com
local         	http://127.0.0.1:8879/charts
kubefed-charts	https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts```
```

We can verify that the KubeFed chart is now available using this Helm search command

```
$ helm search kubefed
NAME                        	CHART VERSION	APP VERSION	DESCRIPTION
kubefed-charts/federation-v2	0.0.10
```

Update your repo 

```
$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kubefed-charts" chart repository
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈ Happy Helming!⎈

```

Then installation is as simple as:

```
$ helm install --name kubefed kubefed-charts/federation-v2 -n kube-federation-system
```

## Registering clusters with the federation

Once the control plane is installed make sure your ~/.kube/config file has contexts for the host cluster and all the member clusters and then you can use the `kubefedctl join` command to add clusters to the federation:

```
$ kf join cluster1 --cluster-context cluster-1 --host-cluster-context cluster-1 --v=2
$ kf join cluster2 --cluster-context cluster-2 --host-cluster-context cluster-1 --v=2
```

Note that cluster-1 here is the host cluster, but is also registered as a member cluster. This is totally fine. There is no need to have a dedicated host cluster. 

To check the status of the federation you can get the kubefedclusters CRDs:

```
$ kubectl -n kube-federation-system get kubefedclusters
  
  NAME       READY   AGE
  cluster-1   True    1m
  cluster-2   True    1m
```

You use kubectl here and not kubefedctl.

If you want to unregister a cluster from the federation use the kubefedctl `unjoin` command.

```
$ kf unjoin cluster-2 --cluster-context cluster-2 --host-cluster-context cluster-1 --v=2
```

## Working with federated API types

Kubernetes federation V1 supported only a limited number of Kubernetes API types. With KubeFed V2 any API type can be federated including your own CRDs. But, it doesn't happen automatically. You need to enable first any type you want to federate.

```
$ kf enable <API Type>
```

The specification of the type to enable is pretty flexible. It can be the kind of type, the plural name, the group qualified plural name or the short name. For example for deployments it can be anyone of Deployment (kind), deploy (short name) , deployments (plural) or deployment.apps (group-qualified plural name).


When you enable a type kubefedctl generates a `Federated<Type>` CRDs (e.g. FederatedDeployment) and a `Federated<Type>Config` association to the original type (Deployment).

Note that the target type must be installed on all member clusters. Ideally, all clusters should run the same version of Kubernetes and be upgraded in tandem to avoid versioning issues. Even if all clusters run the same version of Kubernetes they might not have the same CRDs installed. Remember you can federate CRDs too, but only as long as they are installed on all clusters.

Suppose you have a CRD called awesome in the API group example.com. You can verify it is installed in cluster-1 and cluster-2 by running this little script:

```
CLUSTER_CONTEXTS="cluster-1 cluster-2"
for c in ${CLUSTER_CONTEXTS}; do
    echo ----- ${c} -----
    kubectl --context=${c} api-resources --api-group=example.com
done
```

The result should be:

```
----- cluster1 -----
NAME   SHORTNAMES   APIGROUP      NAMESPACED   KIND
awesome                example.com   true      Awesome
----- cluster2 -----
NAME   SHORTNAMES   APIGROUP      NAMESPACED   KIND
awesome                                        Awesome
``` 

## Federating resources

Enabling API types for federation doesn't actually distribute any resources across the clusters. When you are ready to propagate resources across your federation you can use the `federate` command:

```
kubefedctl federate <TYPE NAME> <RESOURCE NAME> [flags]
``` 

Let's see the output f federating a pod without actually federating it (similar to a dry run). Here is the command:

```
$ kf federate pod trouble --output  yaml
```

Here are selected parts of the generated output. First the API version is types.kubefed.io/v1beta1 and the kind is FederatedPod. 

```
apiVersion: types.kubefed.io/v1beta1
kind: FederatedPod
metadata:
  name: trouble
  namespace: default
``` 

Then, comes the spec that contains the placement with its clusterSelector in case you want to federate only to clusters that match some criteria.
  
```
spec:
  placement:
    clusterSelector:
      matchLabels: {}
```

The rest of the spec is a standard pod template:

```
  template:
    metadata:
      labels:
        run: trouble
    spec:
      containers:
      - args:
        - bash
        image: g1g1/py-kube:0.2
        imagePullPolicy: IfNotPresent
        name: trouble        
        resources: {}
        ...
``` 

By default the federated resource will be created in the same namespace as the target type. The API type must be enabled and installed in all the federation clusters of course. 

### Federating an entire namespace

KubeFed supports whole namespace federation too. This is very useful because namespaces are convenient for organizing groups of resources together and it often makes sense to federate all the resources in a namespace in one fell swoop. Conceptually a namespace is also a Kubernetes resource, so you can think of it as just federating a single resource, which is the namespace. But, in practice namespaces are different from other resources because they are the only resource that contains other resources. The key for federating namespace is the --contents flag that is required. You can also exclude some resources from federation using the `--skip-api-resources` flag with a coma-separated list of resources.  

```
kubefedctl federate namespace awesome-namespace --contents --skip-api-resources "secrets,apps"
```

### Checking status of federated resources

The top-level `status` field of a federated resource contains information about the propagation of the resource across the federation member clusters. Here is an example:

```
apiVersion: types.kubefed.io/v1beta1
kind: FederatedNamespace
metadata:
  name: awesome-namespace
  namespace: awesome-namespace
spec:
  placement:
    clusterSelector: {}
status:
  # The status True of the condition of type Propagation
  # indicates that the state of all member clusters is as
  # intended as of the last probe time.
  conditions:
  - type: Propagation
    status: True
    lastTransitionTime: "2019-12-08T14:33:45Z"
    lastUpdateTime: "2019-12-08T14:33:45Z"
  # The namespace 'awesome-namespace' has been verified to exist in the
  # following clusters as of the lastUpdateTime recorded
  # in the 'Propagation' condition. Since that time, no
  # change has been detected to this resource or the
  # resources it manages.
  clusters:
  - name: cluster-1
  - name: cluster-2
``` 

## Using overrides

In the real world not all clusters are the same. You may need to make various cluster-specific changes. The overrides field of the FederatedDeployment allow you to do exactly that. You specify the overrides using the [jsonpatch](http://jsonpatch.com/) syntax, similar to Kustomize.   


For each override you specify a path (e.g. /spec/replicas) and then either you provide a value to set (e.g. `value: 5`) or an operation (e.g. "add" or "remove") and a value to apply. Here is an example:

```
kind: FederatedDeployment
...
spec:
  ...
  overrides:
  # Apply overrides to cluster1
    - clusterName: cluster1
      clusterOverrides:
        # Set the replicas field to 5
        - path: "/spec/replicas"
          value: 5
        # Set the image of the first container
        - path: "/spec/template/spec/containers/0/image"
          value: "nginx:1.17.0-alpine"
        # Ensure the annotation "foo: bar" exists
        - path: "/metadata/annotations"
          op: "add"
          value:
            foo: bar
        # Ensure an annotation with key "baz" does not exist
        - path: "/metadata/annotations/baz"
          op: "remove"
```

## Using placement to control federation

The placement field of the federated resources controls which member cluster the resource will be federated to. There two ways to place federated resources:
 
- direct list of cluster names
- label-based cluster selector

Here is a direct list placement:

```
spec:
  placement:
    clusters: 
      - name: cluster-1
      - name: cluster-2
```

And, here is label-based cluster selector that deployed the resource to clusters that have a label `federate: True`

```
spec:
  placement:
    clusterSelector:
      matchLabels:
        federate: True 
```

So far so good, but there's more. If there is no placement field or a placement with empty clusterSelector then the resource will be placed in all member clusters:

```
spec:
  placement:
    clusterSelector: {}
```

However, if an empty list of clusters is specified then the resource will not be deployed to ANY cluster!

```
spec:
  placement:
    clusters: []
```

In general, a list of clusters if specified always overrides the cluster selector. In this case, the resource will not be deployed to any cluster because of the empty list of clusters, not even to clusters that have a matching label:

```
spec:
  placement:
    clusters: []
    clusterSelector:
      matchLabels:
        federate: True 

```

## Debugging propagation failures

You can use kubectl and the same techniques you use for debugging general Kubernetes issues. The kubectl describe command will show you events related to a federated resource:

```
$ kubectl describe <federated CRD> <CR name> -n awesome-namespace
```

If that doesn't help then you can check the federation controller logs:

```
$ kubectl logs deployment/kubefed-controller-manager -n kube-federation-system
```

Now, that you have a good sense of how to work with federated resources let's look at higher abstractions that are built on top of the basics.
  
## Employing Higher-Order Behavior

There several high-level multi-cluster considerations and patterns that KubeFed supports out of the box. Those behaviors are built-on top of the foundation building blocks of templates, overrides and placement. Let's review them.  

### Utilizing multi-cluster Ingress DNS

Ingress in a single cluster is done at the edge of the cluster and forwards traffic into the cluster. Ingress literally means entrance. But, in multi-cluster world the situation is different the cluster it may be needed to send requests from the receiving cluster to a different cluster. Finding the correct destination relies on external DNS, which is used in addition to the in-cluster CoreDNS. The primary idea is that endpoints from all the clusters are managed by a DNS endpoint controller and an Ingress DNS controller that watch all the clusters and update the multi-cluster IngressDNSRecord and domain names. An external DNS controller interacts with the external DNS provider to assign external names that are valid across all the clusters and allow locating endpoints across clusters.

The following diagram illustrates the flow of information and control loops:

![](images/chapter 11 - multi cluster ingress dns.png)
**images/chapter 11 - multi cluster ingress dns.png**  

### Utilizing multi-cluster Service DNS

In a Kubernetes federation services need to be federated, which means their backing pods may be federated across multiple clusters. In order to access those pods and their endpoints federated services require a mechanism very similar to the multi-cluster ingress DNS.

The typical workflow is:

1. Create a FederatedDeployment and FederatedService objects. 
2. Create a Domain object that associates a DNS zone and authoritative name server for the KubeFed control plane.
3  Create a ServiceDNSRecord object that identifies the intended domain name of a multi-cluster Service object.
4. DNS Endpoint controller will create a DNSEndpoint object associated with the ServiceDNSRecord. It contains 3 A records: 
   - <service>.<namespace>.<federation>.svc.<federation-domain> 
   - <service>.<namespace>.<federation>.svc.<region>.<federation-domain> 
   - <service>.<namespace>.<federation>.svc.<availability-zone>.<region>.<federation-domain>
5. An external DNS system watches and lists DNSEndpoint objects and creates DNS resource records in external DNS providers.
 
The following diagram illustrates the process:

![](images/chapter 11 - multi cluster service dns.png)
**images/chapter 11 - multi cluster service dns.png**


Next, let's look at the critical topic of multi-cluster scheduling.

### Utilizing multi-cluster scheduling

Consider what it means to do multi-cluster scheduling. You need to specify for each deployment the total number of replicas just like on a single cluster, but you also need to provide some constraints and guidance for distributing the pods across the difference clusters. Enter the ReplicaSchedulingPreference. This resource allows you to specify all your preferences and accomplish healthy distribution of your workloads across all the federation cluster.

Here is a simple example that just specifies the total number of 15 replicas. KubeFed will try to distribute the 15 replicas evenly across all member clusters:

```
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: awesome-deployment
  namespace: awesome-ns
spec:
  targetKind: FederatedDeployment
  totalReplicas: 15
``` 

If there are 3 clusters in the federation then 5 replicas will run in each cluster.

The following example is a little more elaborate and uses weighted distribution. The weights are 2:3 for clusters cluster-1 and cluster-2. This means that the 15 replicas will be distributed in a ratio of 2 to 3, which results in 6 replicas running in cluster-1 and 9 replicas running in cluster-2. 

```
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: awesome-deployment
  namespace: awesome-ns
spec:
  targetKind: FederatedDeployment
  totalReplicas: 15
  clusters:
    cluster-1:
      weight: 2
    cluster-2:
      weight: 3
```

Weighted distribution is nice, but it can lead to undesirable edge cases, especially when one of the member clusters becomes unreachable or otherwise unavailable. To main some boundaries you can specify for each cluster a range of the minimal and maximal number of replicas that are allowed to run in the cluster. KubeFed will do its best to maintain the weighted distribution without violating the constraints for min and max number of replicas. In particular the max number is a hard limit that KubeFed will always respect. The minimum number might be impossible to uphold under certain circumstances.

In the following example, the same 2:3 ratio of 15 replicas is maintained. However, cluster-1 has a maxReplicas limit of 5, so it will run just 5 replicas and not run 6 as before. On the other hand cluster-2 has a maxReplicas limit of 12 so it can pick up the slack and run 10 replicas, which is one more replicas than before. The end result is that all 15 replicas are scheduled, cluster-1 runs 5 replicas and cluster-2 runs 10 replicas, which is a ratio of 1:2 and not 2:3. That's the best KubeFed can do under this particular set of constraints, while still scheduling all 15 replicas. 

```
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: awesome-deployment
  namespace: awesome-ns
spec:
  targetKind: FederatedDeployment
  totalReplicas: 15
  clusters:
    cluster-1:
      weight: 2
      minReplicas: 4
      maxReplicas: 5
    cluster-2:
      weight: 3
      minReplicas: 4
      maxReplicas: 12
```

You can also do uniform distribution with exceptions. For example, distribute evenly across all clusters except one particular cluster that has some constraints. Here 100 replicas will be distributed evenly to all clusters, except that cluster-3 must have at least 5 replicas:

```
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: awesome-deployment
  namespace: awesome-ns
spec:
  targetKind: FederatedDeployment
  totalReplicas: 100
  clusters:
    "*":
      weight: 1
    cluster-3:
      minReplicas: 5
      weight: 1
```

Cluster federation shines when you want to treat your multi-cluster system as one big cluster. But, in many cases the correct level of abstraction is a collection of separate clusters. This is where the Gardener project comes in.

# Introducing the Gardener project

The [gardener](https://gardener.cloud/) project is an open source project developed by SAP. It lets you manage thousands (yes, thousands!) of Kubernetes clusters efficiently and economically. Gardener solves a very complex problem and the solution is elegant, but not simple. In this section we will cover the terminology of gardener, its conceptual model, dive deep into its architecture and learn about its extensibility features. The primary theme of gardener is to use Kubernetes to manage Kubernetes clusters. A good way to think about Gardener is Kubernetes-control-plane-as-a-service. 

## Understanding the terminology of Gardener

The gardener project as you may have guessed uses botanical terminology to describe the world. There is a garden, which is a Kubernetes cluster responsible for managing seed clusters. A seed is a Kubernetes cluster responsible for managing a set of shoot clusters. A shoot cluster is a Kubernetes cluster that runs actual workloads. The cool idea behind Gardener is that the shoot clusters contain only the worker nodes. The control planes of all the shoot clusters run as Kubernetes pods and services in the seed cluster.

The following diagram describes in detail the structure of gardener and the relationships between its components:

![](images/chapter 11 - gardener architecture)
**images/chapter 11 - gardener architecture**

Don't panic! underlying all this complexity is a crystal clear conceptual model.

## Understanding the conceptual model of Gardener

The architecture diagram of Gardener can be overwhelming. Let's unpack slowly and surface the underlying principles. Gardener really embraces the spirit of Kubernetes and offloads a lot of the complexity of managing a large set of Kubernetes cluster to Kubernetes itself. At its heart gardener is an aggregated API server that manages a set of custom resources using various controllers. It embraces and takes full advantage of Kubernetes extensibility. This approach is common in the Kubernetes community. Define a set of custom resources and let Kubernetes manage them for you. The novelty of Gardener is that it takes this approach to the extreme and abstracts away parts of Kubernetes infrastructure itself.

In a "normal" Kubernetes cluster the control plane runs in the same cluster as the worker nodes. Typically, in large clusters control plane components like the Kubernetes API server and etcd run on dedicated nodes and don't mix up with the worker nodes. Gardener thinks in terms of many clusters and it takes all the control planes of all the shoot clusters and have a seed cluster to manage them. So Kubernetes control plane of the shoot clusters are managed in the seed cluster as regular Kubernetes Deployments, which automatically provides replication, monitoring, self-healing and rolling updates by Kubernetes.

So, the control plane of a Kubernetes shoot cluster is analogous to a Deployment. The seed cluster on the other hand maps to a Kubernetes node. It manages multiple shoot clusters. It is recommended to have a seed cluster per cloud provider. The gardener developers actually work on a gardenlet controller for seed clusters that is similar to the kubelet on nodes.

If the seed clusters are like Kubernetes nodes, then the Garden cluster that manages those seed clusters is like a Kubernetes cluster that manages its worker nodes.

By pushing the Kubernetes model this far the Gardener project leverages the strengths of Kubernetes to achieve robustness and performance that would be very difficult to build from scratch.

Let's dive in to the architecture.

## Diving into the Gardener architecture

Gardener creates a Kubernetes namespace in the seed cluster for each shoot cluster. It manages the certificates of the shoot cluster as Kubernetes secrets in the seed cluster.  
### Managing cluster state

The etcd data store for each cluster is deployed as a StatefulSet with one replica. In addition events are stored in a separate etcd instance. The etcd data is periodically snapshoted and stored in remote storage for backup and restore purposes. This enables very fast recovery of clusters that lost their control plane (e.g. when an entire seed cluster becomes unreachable). Note that when a seed cluster goes down the shoot cluster continue to run their as usual.

### Managing the control plane

As mentioned before the control plane of a shoot cluster X runs in a separate seed cluster while the worker nodes run in a shoot cluster. This means that pods in the shoot cluster can use internal DNS to locate each other, but communication to the Kubernetes API server running in the seed cluster must be done though external DNS. This means the Kubernetes API server runs as a Service of type LoadBalancer.

### Preparing the infrastructure

When creating a new shoot cluster it's important to provision the necessary infrastructure. Gardener uses terraform for this task. It generates a terraform script based on the shoot cluster specification and stores it as a ConfigMap in the seed cluster. A dedicated Terraformer component runs as a Job and performs all the provisioning and then writes the state into a separate ConfigMap. 

### Using the Machine controller manager

To provision nodes in a provider-agnostic that can work for private clouds too Gardener has several custom resources such as MachineDeployment, MachineClass, MachineSet and Machine. They work with the Kubernetes Cluster Lifecycle group to unify their abstractions because there is a lot of overlap. In addition, Gardener takes advantage of the cluster auto-scaler to offload the complexity of scaling node pools up and down.

### Networking across clusters

The seed cluster and shoot clusters can run on different cloud providers. The worker nodes in the shoot clusters are often deployed in private networks. Since the control plane needs to interact closely with the worker nodes (mostly the kubelet) the Gardener creates a VPN for direct communication.

### Monitoring clusters

Observability is a big part of operating complex distributed systems. Gardener provides a lot of monitoring out of the box using best of class open source projects like a central [Prometheus](https://github.com/prometheus/prometheus) server, deployed in the garden cluster that collects information about all seed clusters. In addition, each shoot cluster gets its own prometheus instance in the seed cluster. To collect metrics gardener deploys two [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) instances for each cluster (one for control plane in the seed and one for the worker nodes in the shoot). The [node-exporter](https://github.com/prometheus/node_exporter) is deployed too to provide additional information of the nodes. The Prometheus [AlertManager]() is used to notify operator when something goes wrong. [Grafana](http://github.com/grafana/grafana) is used to display dashboards with relevant data on the state of the system.

### The gardenctl CLI

You can manage gardener using only kubectl, but you will have to switch profiles and contexts a lot as you explore different clusters. Gardener provides the `gardenctl` command-line tool that offers higher level abstractions and can operate on multiple clusters at the same time. Here is an example:

```
$ gardenctl ls shoots
projects:
- project: team-a
  shoots:
  - dev-eu1
  - prod-eu1

$ gardenctl target shoot prod-eu1
[prod-eu1]

$ gardenctl show prometheus
NAME           READY     STATUS    RESTARTS   AGE       IP              NODE
prometheus-0   3/3       Running   0          106d      10.241.241.42   ip-10-240-7-72.eu-central-1.compute.internal

URL: https://user:password@p.prod-eu1.team-a.seed.aws-eu1.example.com
```
   
One of the most prominent features of Gardener is its extensibility. It has large surface area and it supports many environment. Let's see how extensibility is built into its design. 

## Extending Gardener
 
Gardener supports the following environments:

- AWS
- GCP
- Azure
- AliCloud
- Packet
- OpenStack

It started like Kubernetes itself with a lot of provider-specific support in primary Gardener repository. Over time it followed the Kubernetes example that externalized cloud providers and migrated the providers to a separate gardener-extensions. Providers can be specified using a CloudProfile CRD such as:

```
apiVersion: gardener.cloud/v1alpha1
kind: CloudProfile
metadata:
  name: aws
spec:
  type: aws
# caBundle: |
#   -----BEGIN CERTIFICATE-----
#   ...
#   -----END CERTIFICATE-----
  dnsProviders:
  - type: aws-route53
  - type: unmanaged
  kubernetes:
    versions:
    - 1.12.1
    - 1.11.0
    - 1.10.5
  machineTypes:
  - name: m4.large
    cpu: "2"
    gpu: "0"
    memory: 8Gi
  # storage: 20Gi   # optional (not needed in every environment, may only be specified if no volumeTypes have been specified)
  ...
  volumeTypes:      # optional (not needed in every environment, may only be specified if no machineType has a `storage` field)
  - name: gp2
    class: standard
  - name: io1
    class: premium
  providerConfig:
    apiVersion: aws.cloud.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    constraints:
      minimumVolumeSize: 20Gi
      machineImages:
      - name: coreos
        regions:
        - name: eu-west-1
          ami: ami-32d1474b
        - name: us-east-1
          ami: ami-e582d29f
      zones:
      - region: eu-west-1
        zones:
        - name: eu-west-1a
          unavailableMachineTypes: # list of machine types defined above that are not available in this zone
          - name: m4.large
          unavailableVolumeTypes:  # list of volume types defined above that are not available in this zone
          - name: gp2
        - name: eu-west-1b
        - name: eu-west-1c
```

Then a shoot cluster will choose a provider and configure it with necessary information:

```
apiVersion: gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: johndoe-aws
  namespace: garden-dev
spec:
  cloudProfileName: aws
  secretBindingName: core-aws
  cloud:
    type: aws
    region: eu-west-1
    providerConfig:
      apiVersion: aws.cloud.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vpc: # specify either 'id' or 'cidr'
        # id: vpc-123456
          cidr: 10.250.0.0/16
        internal:
        - 10.250.112.0/22
        public:
        - 10.250.96.0/22
        workers:
        - 10.250.0.0/19
      zones:
      - eu-west-1a
    workerPools:
    - name: pool-01
    # Taints, labels, and annotations are not yet implemented. This requires interaction with the machine-controller-manager, see
    # https://github.com/gardener/machine-controller-manager/issues/174. It is only mentioned here as future proposal.
    # taints:
    # - key: foo
    #   value: bar
    #   effect: PreferNoSchedule
    # labels:
    # - key: bar
    #   value: baz
    # annotations:
    # - key: foo
    #   value: hugo
      machineType: m4.large
      volume: # optional, not needed in every environment, may only be specified if the referenced CloudProfile contains the volumeTypes field
        type: gp2
        size: 20Gi
      providerConfig:
        apiVersion: aws.cloud.gardener.cloud/v1alpha1
        kind: WorkerPoolConfig
        machineImage:
          name: coreos
          ami: ami-d0dcef3
        zones:
        - eu-west-1a
      minimum: 2
      maximum: 2
      maxSurge: 1
      maxUnavailable: 0
  kubernetes:
    version: 1.11.0
    ...
  dns:
    provider: aws-route53
    domain: johndoe-aws.garden-dev.example.com
  maintenance:
    timeWindow:
      begin: 220000+0100
      end: 230000+0100
    autoUpdate:
      kubernetesVersion: true
  backup:
    schedule: "*/5 * * * *"
    maximum: 7
  addons:
    kube2iam:
      enabled: false
    kubernetes-dashboard:
      enabled: true
    cluster-autoscaler:
      enabled: true
    nginx-ingress:
      enabled: true
      loadBalancerSourceRanges: []
    kube-lego:
      enabled: true
      email: john.doe@example.com
```    

But, the extensibility goals of Gardener go far beyond just being provider agnostic. The overall process of standing up a Kubernetes cluster involves many steps. The gardener project aims to let the operator customize each and every step by defining custom resources and web hooks. Here is the general flow diagram with the CRDs, mutating/validating admission controllers and web hooks associated with each step:

![](images/chapter 11 - cluster creation flow)
**images/chapter 11 - cluster creation flow**

Here the CRD categories that comprise the extensibility space of Gardener: 

- DNS providers (e.g., Route53, CloudDNS)
- Blob storage providers (e.g. S3, GCS, ABS)
- Infrastructure providers (e.g. AWS, GCP, Azure)
- Operating systems (e.g. CoreOS Container Linux, Ubuntu, FlatCar Linux)
- Network plugins (e.g. Calico, Flannel, Cilium)
- Non-essential extensions (e.g. Let’s Encrypt certificate service)

## Gardener ring

Another novel idea is creating a cluster ring of at least 3 clusters where shoot clusters serve as seed clusters for the next cluster in the ring. Together with the ability to migrate control planes to other cluster the ring provides a robust solution that can self-heal if any cluster becomes unavailable. This is especially powerful if clusters are deployed on different cloud providers or at least different regions. It has the potential to protect the garden from severe situations like a total region outage or even a complete cloud provider outages.

Here is how the ring is organized:

![](images/chapter 11 - garden ring.png)
**images/chapter 11 - garden ring.png**

# Summary

In this chapter, we've covered the important topics of Kubernetes cluster federation as well as management of many Kubernetes clusters using the Gardener project. Cluster federation is still in  beta and is a little raw, but it is already usable. There aren't a lot of deployments and the officially supported target platforms are currently AWS and GCE/GKE, but there is a lot of momentum behind cloud federation. It is a very important piece for building massively scalable systems on Kubernetes. We've discussed the motivation and use cases for Kubernetes cluster federation, the federation control plane components, and the federated Kubernetes objects. We also looked into the less supported aspects of federation such as custom scheduling, federated data access, and auto-scaling. We then looked at how to run multiple Kubernetes clusters, which includes setting up a Kubernetes cluster federation, adding and removing clusters to the federation along with load balancing, federated failover when something goes wrong, service discovery, and migration. Then, we dived into running federated workloads across multiple clusters with federated services and the various challenges associated with this scenario.

The gardener project has a very interesting approach and architecture. It tackles the problem of multiple cluster from a different angle and focuses on large-scale management of clusters. It is relatively young, but it is used at scale by SAP and other partners.

At this point, you should have a clear understanding of the current state of federation, what it takes to utilize the existing capabilities provided by Kubernetes, and what pieces you'll have to implement yourself to augment incomplete or immature features. Depending on your use case, you may decide that it's still too early or that you want to take the plunge. The developers working on Kubernetes federation are moving fast, so it's very likely that it will be much more mature and battle-tested by the time you need to make your decision.

If you're are in a position where you need to manage more than a few Kubernetes clusters the Gardener project may be for you.

In the next chapter, we will explore the exciting world of serverless computing on Kubernetes. Serverless can mean two different things: you don't have to manage servers for your long running workloads and also running functions on a service. Both forms of serverless are available for Kubernetes and both of them are exteremely useful.
